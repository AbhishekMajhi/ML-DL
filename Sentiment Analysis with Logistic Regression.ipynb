{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "schema_names": [
        "NLPC1-1"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Copy of C1W1_A1_Logistic Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ahp8SiuTMlZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b043bd55-ae0d-4da4-a059-b02756df3f19"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qECctpKeTpKR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dea13e96-572d-42df-b285-71241f3d8caa"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/workspace')\n",
        "os.getcwd()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/workspace'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdab9n8nS-Ky"
      },
      "source": [
        "## Import functions and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtNeFMZ0S-K3"
      },
      "source": [
        "# run this cell to import nltk\n",
        "import nltk\n",
        "from os import getcwd\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mDdPYGeUP-U",
        "outputId": "2c3080ea-51fe-4d3b-eb8e-6fc751eae2cb"
      },
      "source": [
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZFHFeXaS-K8"
      },
      "source": [
        "\n",
        "\n",
        "Check out the [documentation for the twitter_samples dataset](http://www.nltk.org/howto/twitter.html).\n",
        "\n",
        "\n",
        "#### Import some helper functions that we provided in the utils.py file:\n",
        "* `process_tweet()`: cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\n",
        "* `build_freqs()`: this counts how often a word in the 'corpus' (the entire set of tweets) was associated with a positive label '1' or a negative label '0', then builds the `freqs` dictionary, where each key is a (word,label) tuple, and the value is the count of its frequency within the corpus of tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScOrVN3IIwV-"
      },
      "source": [
        "\n",
        "def process_tweet(tweet): # tweet: a string containing a tweet\n",
        "    \n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "                word not in string.punctuation):  # remove punctuation\n",
        "            # tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean  #  a list of words containing the processed tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bG7f45DJXJj"
      },
      "source": [
        "\n",
        "def build_freqs(tweets, ys):\n",
        "    # tweets: a list of tweets\n",
        "    # ys: an m x 1 array with the sentiment label of each tweet (either 0 or 1)\n",
        "            \n",
        "  \n",
        "    # Converting np array to list since zip needs an iterable.\n",
        "    # The squeeze is necessary or the list ends up with one element.\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "    freqs = {}\n",
        "    for y, tweet in zip(yslist, tweets):  # looping over all tweets\n",
        "    # and over all processed words in each tweet.\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "\n",
        "    return freqs  # a dictionary mapping each (word, sentiment) pair to its frequency"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vim3lJcaS-LI"
      },
      "source": [
        "### Prepare the data\n",
        "* The `twitter_samples` contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets.  \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQxTZlmXWRtt"
      },
      "source": [
        "# creating samples for all positive and all negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pdj0XTyPS-LN"
      },
      "source": [
        "* Train test split: 20% will be in the test set, and 80% in the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WPaHcreXBDA"
      },
      "source": [
        "# train and test split\n",
        "\n",
        "test_pos = all_positive_tweets[4000:]    # start from 4000\n",
        "train_pos = all_positive_tweets[:4000]   # remaining means upto 4000\n",
        "\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "# Then sum them up!!\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkVYeFEaS-LS"
      },
      "source": [
        "* Numpy array of positive labels and negative labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpcaK0dKXq_1"
      },
      "source": [
        "# Do them by combining...\n",
        "\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis = 0)\n",
        "# Here we took all np.ones for postive labels and np.zeros for negative labels..\n",
        "\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis = 0)\n",
        "\n",
        "\n",
        "#  train_y len is 8000\n",
        "# test_y len is 2000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43Lx-xp6S-Lb",
        "outputId": "adb726a4-7a36-482b-9b95-b0317dafb593"
      },
      "source": [
        "# Print the shape train and test sets\n",
        "print(\"train_y.shape = \" + str(train_y.shape))\n",
        "print(\"test_y.shape = \" + str(test_y.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_y.shape = (8000, 1)\n",
            "test_y.shape = (2000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oHNAmuuS-Lf",
        "outputId": "b3b648eb-73aa-4782-e459-13ed6400cfe7"
      },
      "source": [
        "# create frequency dictionary\n",
        "freqs = build_freqs(train_x, train_y)\n",
        "\n",
        "# check the output\n",
        "print(\"type(freqs) = \" + str(type(freqs)))\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "type(freqs) = <class 'dict'>\n",
            "len(freqs) = 11346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1IA4M3YS-Lk"
      },
      "source": [
        "### Testing Process tweet implementation.\n",
        "The given function `process_tweet()` tokenizes the tweet into individual words, removes stop words and applies stemming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lybdw3QFS-Ll",
        "outputId": "45c6da0c-31ae-4471-f4fd-e8083307a337"
      },
      "source": [
        "# test the function below\n",
        "print('This is an example of a positive tweet: \\n', train_x[100])\n",
        "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[100]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is an example of a positive tweet: \n",
            " @metalgear_jp @Kojima_Hideo I want you're T-shirts ! They are so cool ! :D\n",
            "\n",
            "This is an example of the processed version of the tweet: \n",
            " ['want', 't-shirt', 'cool', ':D']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d05S3hnFS-Lo"
      },
      "source": [
        "# Logistic regression \n",
        "\n",
        "\n",
        "### Sigmoid\n",
        "* The sigmoid function is defined as: \n",
        "\n",
        "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
        "\n",
        "It maps the input 'z' to a value that ranges between 0 and 1, and so it can be treated as a probability. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSoqx1UjS-Ls"
      },
      "source": [
        "def sigmoid(z): #  z: is the input (can be a scalar or an array)\n",
        "\n",
        "    # Sigmoid of z\n",
        "    h = 1 / (1 + np.exp(-z)) \n",
        "    \n",
        "    return h  # the sigmoid of z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnFjk05ZS-Lv"
      },
      "source": [
        "### Logistic regression: regression and a sigmoid\n",
        "\n",
        "Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.\n",
        "\n",
        "Regression:\n",
        "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
        "\n",
        "Logistic regression\n",
        "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
        "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
        "Here 'z' refered as the 'logits'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgfgS7WDS-Lw"
      },
      "source": [
        "### Cost function and Gradient\n",
        "\n",
        "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
        "\n",
        "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
        "* $m$ is the number of training examples\n",
        "* $y^{(i)}$ is the actual label of the i-th training example.\n",
        "* $h(z(\\theta)^{(i)})$ is the model's prediction for the i-th training example.\n",
        "\n",
        "The loss function for a single training example is\n",
        "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
        "\n",
        "* All the $h$ values are between 0 and 1, so the logs will be negative. That is the reason for the factor of -1 applied to the sum of the two loss terms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apqMlhedS-L0"
      },
      "source": [
        "#### Update the weights\n",
        "\n",
        "Apply gradient descent to iteratively improve your model's predictions.  \n",
        "The gradient of the cost function $J$ with respect to one of the weights $\\theta_j$ is:\n",
        "\n",
        "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x_j \\tag{5}$$\n",
        "* 'i' is the index across all 'm' training examples.\n",
        "* 'j' is the index of the weight $\\theta_j$, so $x_j$ is the feature associated with weight $\\theta_j$\n",
        "\n",
        "* To update the weight $\\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\\alpha$:\n",
        "$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n",
        "* The learning rate $\\alpha$ is a value that we choose to control how big a single update will be.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct7NWmqFS-L2"
      },
      "source": [
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "\n",
        "    m = x.shape[0]  # the number of rows in matrix x\n",
        "    \n",
        "    for i in range(0, num_iters):\n",
        "        \n",
        "        # z is the dot product of x and theta\n",
        "        z = np.dot(x,theta)\n",
        "        \n",
        "        # apply sigmoid on 'z'\n",
        "        h = sigmoid(z)\n",
        "        \n",
        "        # Cost function\n",
        "        J = -1./m * (np.dot(y.T, np.log(h)) + np.dot((1-y).T,np.log(1-h)))                                                    \n",
        "\n",
        "        # weights update\n",
        "        theta = theta - (alpha/m) * np.dot(x.T,(h-y))\n",
        "\n",
        "    J = float(J)\n",
        "    return J, theta  # J is the final cost and theta is the final weight vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4lwj1zJS-L5"
      },
      "source": [
        "## Feature Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0JGg5IhS-L6"
      },
      "source": [
        "def extract_features(tweet, freqs):\n",
        "  # Here tweet is a list of words for one tweet.\n",
        "  # And freqs is a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "\n",
        "    # process tweets\n",
        "    word_l = process_tweet(tweet)\n",
        "    \n",
        "    # 3 elements in the form of a 1 x 3 vector\n",
        "    x = np.zeros((1, 3)) \n",
        "    \n",
        "    #bias term is set to 1\n",
        "    x[0,0] = 1 \n",
        "    \n",
        "    # loopping through each word in the list of words\n",
        "    for word in word_l:\n",
        "        \n",
        "        # increment the word count for the positive label 1\n",
        "        x[0,1] += freqs.get((word, 1.0),0)\n",
        "        \n",
        "        # increment the word count for the negative label 0\n",
        "        x[0,2] += freqs.get((word, 0.0),0)\n",
        "\n",
        "    assert(x.shape == (1, 3))\n",
        "    return x  # x is a feature vector of dimension (1,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSyvn0TaS-L-"
      },
      "source": [
        "##  Model Training..\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_twwnntSS-L-"
      },
      "source": [
        "# collect the features 'x' and stack them into a matrix 'X'\n",
        "X = np.zeros((len(train_x), 3))\n",
        "for i in range(len(train_x)):\n",
        "    X[i, :]= extract_features(train_x[i], freqs)\n",
        "\n",
        "# training labels corresponding to X\n",
        "Y = train_y\n",
        "\n",
        "# Apply gradient descent\n",
        "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
        "print(f\"The cost after training is {J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RUKZUZ_S-L_"
      },
      "source": [
        "# Model Testing.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhvHtQNOS-L_"
      },
      "source": [
        "def predict_tweet(tweet, freqs, theta):\n",
        "    # theta is a (3,1) vector of weights\n",
        "\n",
        "    # extract features..\n",
        "    x = extract_features(tweet,freqs)\n",
        "    \n",
        "    # making the prediction using x and theta\n",
        "    y_pred = sigmoid(np.dot(x,theta))\n",
        "    \n",
        "    return y_pred    # the probability of a tweet being positive or negative"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P16nhBWDS-MC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "befaf207-261b-4f6d-8496-e7de57f77d2d"
      },
      "source": [
        "# test area..\n",
        "my_tweet = 'I am learning :)'\n",
        "predict_tweet(my_tweet, freqs, theta)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.81636424]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X6qpj6PS-ME"
      },
      "source": [
        "**Performance testing on val data set.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVIRrPY9S-MF"
      },
      "source": [
        "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
        "\n",
        "    # the list for storing predictions\n",
        "    y_hat = []\n",
        "    \n",
        "    for tweet in test_x:\n",
        "        y_pred = predict_tweet(tweet, freqs, theta)\n",
        "        \n",
        "        if y_pred > 0.5:\n",
        "            # append 1.0 to the list\n",
        "            y_hat.append(1)\n",
        "        else:\n",
        "            # append 0 to the list\n",
        "            y_hat.append(0)\n",
        "\n",
        "    # Here, y_hat is a list, but test_y is (m,1) array\n",
        "    # so we gonna convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
        "    \n",
        "    accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
        "    \n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usuruP4XS-MH"
      },
      "source": [
        "# Error Analysis\n",
        "\n",
        "Lets see where the tweets that our model misclassified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqxNJ1oHS-MH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb70655-bc7f-44ef-bc99-01e0ce3f63ab"
      },
      "source": [
        "print('Label Predicted Tweet')\n",
        "for x,y in zip(test_x,test_y):\n",
        "    y_hat = predict_tweet(x, freqs, theta)\n",
        "\n",
        "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
        "        print('THE TWEET IS:', x)\n",
        "        print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
        "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label Predicted Tweet\n",
            "THE TWEET IS: @jaredNOTsubway @iluvmariah @Bravotv Then that truly is a LATERAL move! Now, we all know the Queen Bee is UPWARD BOUND : ) #MovingOnUp\n",
            "THE PROCESSED TWEET IS: ['truli', 'later', 'move', 'know', 'queen', 'bee', 'upward', 'bound', 'movingonup']\n",
            "1\t0.49996890\tb'truli later move know queen bee upward bound movingonup'\n",
            "THE TWEET IS: @MarkBreech Not sure it would be good thing 4 my bottom daring 2 say 2 Miss B but Im gonna be so stubborn on mouth soaping ! #NotHavingit :p\n",
            "THE PROCESSED TWEET IS: ['sure', 'would', 'good', 'thing', '4', 'bottom', 'dare', '2', 'say', '2', 'miss', 'b', 'im', 'gonna', 'stubborn', 'mouth', 'soap', 'nothavingit', ':p']\n",
            "1\t0.48622857\tb'sure would good thing 4 bottom dare 2 say 2 miss b im gonna stubborn mouth soap nothavingit :p'\n",
            "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots\n",
            "http://t.co/UGQzOx0huu\n",
            "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
            "1\t0.48370665\tb\"i'm play brain dot braindot\"\n",
            "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/aOKldo3GMj http://t.co/xWCM9qyRG5\n",
            "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
            "1\t0.48370665\tb\"i'm play brain dot braindot\"\n",
            "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/R2JBO8iNww http://t.co/ow5BBwdEMY\n",
            "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
            "1\t0.48370665\tb\"i'm play brain dot braindot\"\n",
            "THE TWEET IS: off to the park to get some sunlight : )\n",
            "THE PROCESSED TWEET IS: ['park', 'get', 'sunlight']\n",
            "1\t0.49578765\tb'park get sunlight'\n",
            "THE TWEET IS: @msarosh Uff Itna Miss karhy thy ap :p\n",
            "THE PROCESSED TWEET IS: ['uff', 'itna', 'miss', 'karhi', 'thi', 'ap', ':p']\n",
            "1\t0.48199810\tb'uff itna miss karhi thi ap :p'\n",
            "THE TWEET IS: @phenomyoutube u probs had more fun with david than me : (\n",
            "THE PROCESSED TWEET IS: ['u', 'prob', 'fun', 'david']\n",
            "0\t0.50020353\tb'u prob fun david'\n",
            "THE TWEET IS: pats jay : (\n",
            "THE PROCESSED TWEET IS: ['pat', 'jay']\n",
            "0\t0.50039294\tb'pat jay'\n",
            "THE TWEET IS: my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
            "THE PROCESSED TWEET IS: ['belov', 'grandmoth']\n",
            "0\t0.50000002\tb'belov grandmoth'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}